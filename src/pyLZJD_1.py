
from pyLZJD import digest
import glob
import numpy as np
import math
import matplotlib.pyplot as plt
import pandas as pd
import json
import pickle
import time
import argparse
import traceback

if __name__ == '__main__':
    def write_list(a_list, name):
        # store list in binary file so 'wb' mode
        try:
            with open(name, 'wb') as fp:
                pickle.dump(a_list, fp)
                print('Done writing list into a binary file')
        except (AttributeError, EOFError, ImportError, IndexError) as e:
            print(traceback.format_exc(e))
        except Exception as e:
            print(traceback.format_exc(e))
            return

    def create_file_index(paths):
        filedict = {}
        k = 0
        try:
            for i in paths:
                filedict[i] = k
                k += 1
        except Exception as e:
            print(traceback.format_exc(e))
        return filedict

    def find_digest(file):
        return digest(file, processes=-1)[0]

    def create_token_freq(x_paths):
        unique_tokens = set()
        tokenindex = {}
        tokenfreqDict = {}
        idx = 0
        token_idx = 0
        batch = len(x_paths)
        if batch==0:
            raise Exception("Incorrect directory path or no file to create term document matrix.")
        printProgressBar(0, batch, prefix='create_token_freq', suffix='Complete', length=50)

        for file in x_paths:
            printProgressBar(idx + 1, batch, prefix='create_token_freq', suffix='Complete', length=50)
            try:
                tokens = tuple(find_digest(file))
            except Exception as e:
                print("Failed to find digest of file.")
                print(traceback.format_exc(e))
            # print(tokens)
            tokenfreqDict[file] = {}
            for i in tokens:
                if i not in unique_tokens:
                    tokenindex[str(i)] = token_idx
                    token_idx += 1
                    unique_tokens.add(i)
                if not tokenfreqDict[file]:
                    tokenfreqDict[file] = [i]
                else:
                    tokenfreqDict[file].append(i)
            idx += 1

        return unique_tokens, tokenfreqDict, tokenindex


    def cosine_similarity(a, b):
        "compute cosine similarity of v1 to v2: (a dot b)/{||a||*||b||)"
        sumxx, sumxy, sumyy = 0, 0, 0
        for i in range(len(a)):
            x = a[i];
            y = b[i]
            sumxx += x * x
            sumyy += y * y
            sumxy += x * y
        return sumxy / math.sqrt(sumxx * sumyy)


    # Print iterations progress
    def printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='â–ˆ', printEnd="\r"):

        percent = ("{0:." + str(decimals) + "f}").format(100 * (iteration / float(total)))
        filled_length = int(length * iteration // total)
        bar = fill * filled_length + '-' * (length - filled_length)
        print(f'\r{prefix} |{bar}| {percent}% {suffix}', end=printEnd)
        # Print New Line on Complete
        if iteration == total:
            print()


    def create_tdm(tokenindex, tokenfreq, filedict):
        tdm = [[int(False) for i in range(len(tokenindex))] for j in range(len(tokenfreq))]
        idx = 0
        batch = len(tokenfreq) + 1
        printProgressBar(0, batch, prefix='create_tdm', suffix='Complete', length=50)
        for key, val in tokenfreq.items():
            printProgressBar(idx + 1, batch, prefix='create_tdm', suffix='Complete', length=50)
            for token in val:
                tdm[filedict[key]][tokenindex[str(token)]] = int(True)
            idx += 1
        np.save('output_v2.csv', tdm)
        printProgressBar(idx + 1, batch, prefix='create_tdm', suffix='Complete', length=50)
        print("finished saving file")
        return tdm


    # Save the similarity index between the documents
    def pair(s):
        for i, v1 in enumerate(s):
            for j in range(i + 1, len(s)):
                yield [v1, s[j]]


    def create_similarity_dict(fileIndex, tdm):
        similarityDict = {}

        l = list(pair([x for x in fileIndex.keys()]))

        batch = len(l)
        idx = 0
        printProgressBar(0, batch, prefix='create_similarity_dict', suffix='Complete', length=50)
        for (a, b) in l:
            printProgressBar(idx + 1, batch, prefix='create_similarity_dict', suffix='Complete', length=50)
            similarityDict[(a, b)] = cosine_similarity(tdm[fileIndex[a]], tdm[fileIndex[b]])
            idx += 1

        jsonVdict = dict((':'.join(k), v) for k, v in similarityDict.items())
        with open("SimilarityDict.json", "w") as outfile:
            json.dump(jsonVdict, outfile)
        return similarityDict


    def plot_cosine_similarity_matrix(fileIndex, similarityDict):
        documents = [x for x in fileIndex.keys()]
        final_df = pd.DataFrame(np.asarray(
            [[(similarityDict[(x, y)] if (x, y) in similarityDict else 0) for y in documents] for x in documents]))
        final_df.columns = documents
        final_df.index = documents
        fig, ax = plt.subplots()
        ax.set_xticks(np.arange(len(documents)))
        ax.set_yticks(np.arange(len(documents)))
        ax.set_xticklabels(documents)
        ax.set_yticklabels(documents)
        ax.matshow(final_df, cmap='seismic')
        for (i, j), z in np.ndenumerate(final_df):
            if z != 0:
                ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center',
                        bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))
            else:
                None
        fig.suptitle('Cosine Similarity Index between the Documents')
        plt.show()
        plt.savefig('foo.png')

    parser = argparse.ArgumentParser()
    parser.add_argument('--input_file_dir', type=str, required=True,help='Add data files directory as argument.')
    args = parser.parse_args()
    inputDir = args.input_file_dir
    path = "./" + inputDir + "/"
    X_paths = glob.glob(path + "*")
    if len(X_paths) == 0:
        raise Exception("Incorrect Directory path or no file to compute term document matrix")
    write_list(X_paths, 'listfile')

    fileIndex = create_file_index(X_paths)
    print("fileIndexCreated")

    tokens, tokenizerDict, tokenized = create_token_freq(X_paths)
    with open("tokenIndex.json", "w") as outfile:
        json.dump(tokenized, outfile)
    print("tokenlistCreated")

    v2s = time.time()
    tdm2 = create_tdm(tokenized, tokenizerDict, fileIndex)
    print("term document matrix Created")
    print("v2 time: ", time.time() - v2s)
