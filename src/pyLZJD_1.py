from pyLZJD import digest, sim
import glob
import numpy as np
import math
import sys,os
from collections import Counter
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics.pairwise import linear_kernel
import json
import pickle
import time

if __name__ == '__main__':

    def createFileIndex(paths):
        filedict={}
        k=0
        for i in paths:
            filedict[i]=k
            k+=1
        return filedict
        
    def findDigest(file):
        return digest(file,processes=-1)[0]
        
    def createTokenFreq(X_paths):
        tokenset=set()
        tokenfreqDict = {}
        idx=0
        batch=len(X_paths)
        printProgressBar(0, batch, prefix = 'createTokenFreq', suffix = 'Complete', length = 50)
        for file in X_paths:
            printProgressBar(idx+1, batch, prefix = 'createTokenFreq', suffix = 'Complete', length = 50)
            tokens=tuple(findDigest(file))
            tokenfreqDict[file] = {}
            for i in tokens:
                tokenset.add(i)
                if not tokenfreqDict[file]:
                    tokenfreqDict[file]=[i]
                else:
                    tokenfreqDict[file].append(i)
            idx+=1
        return (tokenset,tokenfreqDict)
        
    def createTokenFreqV2(X_paths):
        tokenset=set()
        tokenindex={}
        tokenfreqDict = {}
        idx=0
        tidx=0
        batch=len(X_paths)
        printProgressBar(0, batch, prefix = 'createTokenFreq', suffix = 'Complete', length = 50)

        for file in X_paths:
            printProgressBar(idx+1, batch, prefix = 'createTokenFreq', suffix = 'Complete', length = 50)
            tokens=tuple(findDigest(file))
            #print(tokens)
            tokenfreqDict[file] = {}
            for i in tokens:

                if i not in tokenset:
                    tokenindex[str(i)]=tidx
                    tidx+=1
                    tokenset.add(i)
                if not tokenfreqDict[file]:
                    tokenfreqDict[file]=[i]
                else:
                    tokenfreqDict[file].append(i)
            idx+=1

        return (tokenset,tokenfreqDict,tokenindex)
    
    def cosine_similarity(a,b):
        "compute cosine similarity of v1 to v2: (a dot b)/{||a||*||b||)"
        sumxx, sumxy, sumyy = 0, 0, 0
        for i in range(len(a)):
            x = a[i]; y = b[i]
            sumxx += x*x
            sumyy += y*y
            sumxy += x*y
        return sumxy/math.sqrt(sumxx*sumyy)
   
    # Print iterations progress
    def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = "\r"):
        percent = ("{0:." + str(decimals) + "f}").format(100 * (iteration / float(total)))
        filledLength = int(length * iteration // total)
        bar = fill * filledLength + '-' * (length - filledLength)
        print(f'\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)
        # Print New Line on Complete
        if iteration == total: 
            print()
       
        
    def createTDM(tokenlist,tokenfreq,filedict):
        tdm=[[False for i in range(len(tokenlist))] for j in range(len(tokenfreq))]
        idx=0
        batch=len(tokenfreq)
        printProgressBar(0, batch, prefix = 'createTDM', suffix = 'Complete', length = 50)
        for key,val in tokenfreq.items():
            printProgressBar(idx+1, batch, prefix = 'createTDM', suffix = 'Complete', length = 50)
            for token in val:
                tdm[filedict[key]][tokenlist.index(token)]=True
            idx+=1
        np.savetxt('output.csv',tdm,delimiter=",")
        return tdm
    
    def createTDM_v2(tokenindex,tokenfreq,filedict):
        tdm=[[False for i in range(len(tokenindex))] for j in range(len(tokenfreq))]
        idx=0
        batch=len(tokenfreq)+1
        printProgressBar(0, batch, prefix = 'createTDM', suffix = 'Complete', length = 50)
        for key,val in tokenfreq.items():
            printProgressBar(idx+1, batch, prefix = 'createTDM', suffix = 'Complete', length = 50)
            for token in val:
                tdm[filedict[key]][tokenindex[str(token)]]=True
            idx+=1
        #print(len(tdm),len(tdm[0]))
        #print("Finished creating tdm")
        np.savetxt('output_v2.csv',tdm,delimiter=",")
        printProgressBar(idx+1, batch, prefix = 'createTDM', suffix = 'Complete', length = 50)
        print("finished saving file")
        return tdm
                
    # Save the similarity index between the documents
    def pair(s):
        for i, v1 in enumerate(s):
            for j in range(i+1, len(s)):
                yield [v1, s[j]]
                
    def createSimilarityDict(fileIndex,tdm):
        similarityDict={}
        
        l=list(pair([x for x in fileIndex.keys()]))
        
        batch=len(l)
        idx=0
        printProgressBar(0, batch, prefix = 'createSimilarityDict', suffix = 'Complete', length = 50)
        for (a,b) in l:
            printProgressBar(idx+1, batch, prefix = 'createSimilarityDict', suffix = 'Complete', length = 50)
            #c = np.logical_and(tdm[fileIndex[a]], tdm[fileIndex[b]])
            #similarityDict[(a,b)]  = np.count_nonzero(c)/len(c)
            
            similarityDict[(a,b)] = cosine_similarity(tdm[fileIndex[a]], tdm[fileIndex[b]])
            idx+=1
            
        jsonVdict=  dict((':'.join(k), v) for k,v in similarityDict.items())
        with open("SimilarityDict.json", "w") as outfile:
            json.dump(jsonVdict, outfile)
        return similarityDict
        
    def plotCosineSimilarityMatrix(fileIndex,similarityDict):
        documents= [x for x in fileIndex.keys()]
        final_df = pd.DataFrame(np.asarray([[(similarityDict[(x,y)] if (x,y) in similarityDict else 0) for y in documents] for x in documents]))
        final_df.columns =  documents
        final_df.index = documents
        fig, ax = plt.subplots()
        ax.set_xticks(np.arange(len(documents)))
        ax.set_yticks(np.arange(len(documents)))
        ax.set_xticklabels(documents)
        ax.set_yticklabels(documents)
        ax.matshow(final_df, cmap='seismic')
        for (i, j), z in np.ndenumerate(final_df):
          if z != 0 :
            ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center',
                    bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))
          else:
            None
        fig.suptitle('Cosine Similarity Index between the Documents')
        plt.show()
        plt.savefig('foo.png')
                
    def write_list(a_list,name):
        # store list in binary file so 'wb' mode
        with open(name, 'wb') as fp:
            pickle.dump(a_list, fp)
            print('Done writing list into a binary file')
        
    inputDir=sys.argv[1]
    path="./"+inputDir+"/"
    X_paths = glob.glob(path+"*")
    write_list(X_paths,'listfile')
    
    fileIndex=createFileIndex(X_paths)
    print("fileIndexCreated")
    
    #tokenset,tokenfreqDict=createTokenFreq(X_paths)
    tokenset,tokenfreqDict,tokenindex=createTokenFreqV2(X_paths)

    with open("tokenIndex.json", "w") as outfile:
        json.dump(tokenindex, outfile)
    print("tokenlistCreated")
    
    v2s=time.time()
    tdm2=createTDM_v2(tokenindex,tokenfreqDict,fileIndex)
    print("term document matrix Created")
    print("v2 time: " , time.time()-v2s)
    
#    v3s=time.time()
#    tdm3=createTDM_v3(tokenindex,tokenfreqDict,fileIndex)
#    print("term document matrix Created")
#    print("v3 time: " , time.time()-v3s)
#    ts=time.time()
#    similarityDict=createSimilarityDict(fileIndex,tdm2)
#    print("similar time",time.time()-ts)
#    print("similarity dictionary cretated")

    
#    plotCosineSimilarityMatrix(fileIndex,similarityDict)
#    print("Graph plotted")

#    newfilePath="./files/004.html"
#
#    count=int(input("how many most similar file needed"))
#    count=5
#
#
#    tokens=tuple(findDigest(newfilePath))
#    newColumn=[False]*len(tokenlist)
#    print(len(tokenlist))
#    #print(tokens)
#
#    for i in tokens:
#        #print(i)
#        if i in tokenset:
#            newColumn[tokenlist.index(i)]=True
#    #print(newColumn)
#    #print(tdm[-1])
#
#    related_product_indices=findmostsimilarFiles(np.array(newColumn).reshape(1,-1),tdm,count)
#    print(related_product_indices)
#    print(fileIndex)
#    print(np.array(newColumn).reshape(1,-1))
#
