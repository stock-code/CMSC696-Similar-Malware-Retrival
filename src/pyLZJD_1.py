from pyLZJD import digest
import glob
import numpy as np
import math
import matplotlib.pyplot as plt
import pandas as pd
import json
import pickle
import time
import argparse

if __name__ == '__main__':

    def create_file_index(paths):
        filedict = {}
        k = 0
        for i in paths:
            filedict[i] = k
            k += 1
        return filedict


    def find_digest(file):
        return digest(file, processes=-1)[0]


    def create_token_freq(x_paths):
        tokenset = set()
        tokenfreqDict = {}
        idx = 0
        batch = len(x_paths)
        printProgressBar(0, batch, prefix='create_token_freq', suffix='Complete', length=50)
        for file in x_paths:
            printProgressBar(idx + 1, batch, prefix='create_token_freq', suffix='Complete', length=50)
            tokens = tuple(find_digest(file))
            tokenfreqDict[file] = {}
            for i in tokens:
                tokenset.add(i)
                if not tokenfreqDict[file]:
                    tokenfreqDict[file] = [i]
                else:
                    tokenfreqDict[file].append(i)
            idx += 1
        return tokenset, tokenfreqDict


    def create_token_freq_v2(x_paths):
        tokenset = set()
        tokenindex = {}
        tokenfreqDict = {}
        idx = 0
        tidx = 0
        batch = len(x_paths)
        printProgressBar(0, batch, prefix='create_token_freq', suffix='Complete', length=50)

        for file in x_paths:
            printProgressBar(idx + 1, batch, prefix='create_token_freq', suffix='Complete', length=50)
            tokens = tuple(find_digest(file))
            # print(tokens)
            tokenfreqDict[file] = {}
            for i in tokens:

                if i not in tokenset:
                    tokenindex[str(i)] = tidx
                    tidx += 1
                    tokenset.add(i)
                if not tokenfreqDict[file]:
                    tokenfreqDict[file] = [i]
                else:
                    tokenfreqDict[file].append(i)
            idx += 1

        return tokenset, tokenfreqDict, tokenindex


    def cosine_similarity(a, b):
        "compute cosine similarity of v1 to v2: (a dot b)/{||a||*||b||)"
        sumxx, sumxy, sumyy = 0, 0, 0
        for i in range(len(a)):
            x = a[i];
            y = b[i]
            sumxx += x * x
            sumyy += y * y
            sumxy += x * y
        return sumxy / math.sqrt(sumxx * sumyy)


    # Print iterations progress
    def printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='â–ˆ', printEnd="\r"):
        percent = ("{0:." + str(decimals) + "f}").format(100 * (iteration / float(total)))
        filled_length = int(length * iteration // total)
        bar = fill * filled_length + '-' * (length - filled_length)
        print(f'\r{prefix} |{bar}| {percent}% {suffix}', end=printEnd)
        # Print New Line on Complete
        if iteration == total:
            print()


    def create_tdm(tokenlist, tokenfreq, filedict):
        tdm = [[False for i in range(len(tokenlist))] for j in range(len(tokenfreq))]
        idx = 0
        batch = len(tokenfreq)
        printProgressBar(0, batch, prefix='create_tdm', suffix='Complete', length=50)
        for key, val in tokenfreq.items():
            printProgressBar(idx + 1, batch, prefix='create_tdm', suffix='Complete', length=50)
            for token in val:
                tdm[filedict[key]][tokenlist.index(token)] = True
            idx += 1
        np.savetxt('output.csv', tdm, delimiter=",")
        return tdm


    def create_tdm_v2(tokenindex, tokenfreq, filedict):
        tdm = [[False for i in range(len(tokenindex))] for j in range(len(tokenfreq))]
        idx = 0
        batch = len(tokenfreq) + 1
        printProgressBar(0, batch, prefix='create_tdm', suffix='Complete', length=50)
        for key, val in tokenfreq.items():
            printProgressBar(idx + 1, batch, prefix='create_tdm', suffix='Complete', length=50)
            for token in val:
                tdm[filedict[key]][tokenindex[str(token)]] = True
            idx += 1
        # print(len(tdm),len(tdm[0]))
        # print("Finished creating tdm")
        np.savetxt('output_v2.csv', tdm, delimiter=",")
        printProgressBar(idx + 1, batch, prefix='create_tdm', suffix='Complete', length=50)
        print("finished saving file")
        return tdm

    # Save the similarity index between the documents
    def pair(s):
        for i, v1 in enumerate(s):
            for j in range(i + 1, len(s)):
                yield [v1, s[j]]

    def create_similarity_dict(fileIndex, tdm):
        similarityDict = {}

        l = list(pair([x for x in fileIndex.keys()]))

        batch = len(l)
        idx = 0
        printProgressBar(0, batch, prefix='create_similarity_dict', suffix='Complete', length=50)
        for (a, b) in l:
            printProgressBar(idx + 1, batch, prefix='create_similarity_dict', suffix='Complete', length=50)
            # c = np.logical_and(tdm[fileIndex[a]], tdm[fileIndex[b]])
            # similarityDict[(a,b)]  = np.count_nonzero(c)/len(c)

            similarityDict[(a, b)] = cosine_similarity(tdm[fileIndex[a]], tdm[fileIndex[b]])
            idx += 1

        jsonVdict = dict((':'.join(k), v) for k, v in similarityDict.items())
        with open("SimilarityDict.json", "w") as outfile:
            json.dump(jsonVdict, outfile)
        return similarityDict


    def plot_cosine_similarity_matrix(fileIndex, similarityDict):
        documents = [x for x in fileIndex.keys()]
        final_df = pd.DataFrame(np.asarray(
            [[(similarityDict[(x, y)] if (x, y) in similarityDict else 0) for y in documents] for x in documents]))
        final_df.columns = documents
        final_df.index = documents
        fig, ax = plt.subplots()
        ax.set_xticks(np.arange(len(documents)))
        ax.set_yticks(np.arange(len(documents)))
        ax.set_xticklabels(documents)
        ax.set_yticklabels(documents)
        ax.matshow(final_df, cmap='seismic')
        for (i, j), z in np.ndenumerate(final_df):
            if z != 0:
                ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center',
                        bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))
            else:
                None
        fig.suptitle('Cosine Similarity Index between the Documents')
        plt.show()
        plt.savefig('foo.png')


    def write_list(a_list, name):
        # store list in binary file so 'wb' mode
        with open(name, 'wb') as fp:
            pickle.dump(a_list, fp)
            print('Done writing list into a binary file')

    parser = argparse.ArgumentParser()
    parser.add_argument('--input_file_dir', type=str, required=True)
    args = parser.parse_args()
    inputDir = args.input_file_dir
    path = "./" + inputDir + "/"
    X_paths = glob.glob(path + "*")
    write_list(X_paths, 'listfile')

    fileIndex = create_file_index(X_paths)
    print("fileIndexCreated")

    # tokenset,tokenfreqDict=create_token_freq(X_paths)
    tokens, tokenizerDict, tokenized = create_token_freq_v2(X_paths)

    with open("tokenIndex.json", "w") as outfile:
        json.dump(tokenized, outfile)
    print("tokenlistCreated")

    v2s = time.time()
    tdm2 = create_tdm_v2(tokenized, tokenizerDict, fileIndex)
    print("term document matrix Created")
    print("v2 time: ", time.time() - v2s)

#    v3s=time.time()
#    tdm3=createTDM_v3(tokenindex,tokenfreqDict,fileIndex)
#    print("term document matrix Created")
#    print("v3 time: " , time.time()-v3s)
#    ts=time.time()
#    similarityDict=create_similarity_dict(fileIndex,tdm2)
#    print("similar time",time.time()-ts)
#    print("similarity dictionary cretated")


#    plot_cosine_similarity_matrix(fileIndex,similarityDict)
#    print("Graph plotted")

#    newfilePath="./files/004.html"
#
#    count=int(input("how many most similar file needed"))
#    count=5
#
#
#    tokens=tuple(find_digest(newfilePath))
#    newColumn=[False]*len(tokenlist)
#    print(len(tokenlist))
#    #print(tokens)
#
#    for i in tokens:
#        #print(i)
#        if i in tokenset:
#            newColumn[tokenlist.index(i)]=True
#    #print(newColumn)
#    #print(tdm[-1])
#
#    related_product_indices=findmostsimilar_files(np.array(newColumn).reshape(1,-1),tdm,count)
#    print(related_product_indices)
#    print(fileIndex)
#    print(np.array(newColumn).reshape(1,-1))
#
